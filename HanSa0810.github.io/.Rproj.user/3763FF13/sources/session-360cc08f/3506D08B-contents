---
title: 'CS670: Benign vs. DDoS - CIC-IDS2017 Dataset Analysis'
format:
  html:
    code-fold: true
jupyter: python3
---

## Data

For this project I am using a subset of the Intrusion detection evaluation dataset (CID-IDS2017). Specifically, The collected data from the Friday_working_hours_afternoon - DDoS set. The Data is synthetically generated to mimic real-world data (PCAPs). It was made in response to the overall sparsity of reliable test and validation datasets that could effectively train anomaly based intrusion detection approaches.

The CICIDS2017 dataset contains benign and up-to-date (as of 2017) common attacks, including Brute Force, DDoS, Heartbleed etc. For this project I focus on a subset that only contains benign behavior and DDoS attacks.

The Dataset was genereted with realistic background traffic using their proposed B-Profile system to profile abstract behavior and human interactions and then generate benign background traffic. The dataset builds the abstract behavior of 25 users based on the HTTP, HTTPS, FTP, SSH, and email protocols.

It was generated by the University of New Brunswick (UNB), the Canadian Institue of Cybersecurity (CIC).

## 1. The Data Set

The dataset contains 79 features and labels each observation as either 'Benign' or 'DDoS'.

## 1.1 The Data Set

Below displays a few of the feature columns and datapoints for a general overview of PCAPs data along with a bargraph indicating the distribution of Benign and DDoS data points.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import matplotlib.pyplot as plt

# Inspect dataset
# Load dataset (replace 'data.csv' with your dataset file)
df = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')


# Inspect dataset
print(df.head())

def plot_class_frequencies(csv_file, class_column):
    # Load dataset
    df = pd.read_csv(csv_file)

    # Ensure the required column exists
    if class_column not in df.columns:
        print(f"Column '{class_column}' not found in CSV file.")
        return

    # Count occurrences of each class
    class_counts = df[class_column].value_counts()

    # Plot bar chart
    class_counts.plot(kind='bar', color=['blue', 'orange'])
    plt.xlabel(class_column)
    plt.ylabel('Frequency')
    plt.title(f'Frequency of {class_column}')
    plt.xticks(rotation=45)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Show the plot
    plt.show()

plot_class_frequencies('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', ' Label')
```

## 1.2 The Data Set

Throughout my analysis I encode the categorical columns as needed. I use 0 to represent Benign and 1 to represent DDoS in the 'Label' column for classifcation models. I also handle missing or Nan values by calculating the average value of the column and replacing them with it.

## 2. The Questions

For this project I would like to explore the following questions:

1.  Which features are the most important for classifying malicious behavior?
2.  How can I identify these features?
3.  Are there any significant relationships between the most important predictors?
4.  What feature thresholds define a DDoS attack?

The research I do for my master's program involves developing a machine learning/explainable AI tool for network operators to understand the black box nature of models used to predict network attacks. My work involves playing with features included in various training approximation algorithms to alter an explainable decision tree. I think it would be very interesting to examine the features from a data-science perspective to see if I can narrow down the features that classify a particular attack. In this way I could Identify the subset of important features for any of the common attacks included in the full dataset.

## 3. Visualization

The relationships between various features recorded by packet captures are not intuitive. My first step is to visualize the different relationships between the features in the dataset.

## 3.1 Visualization: Correlation Heat Map

First I print all the features contained in the dataset.

I then cleaned the data (as mentioned in 1.2 The Data Set) and created a correlation matrix. I used a heatmap to visualzie the correlation coefficients. For readibility, the squares do not contain the actual correlation values, just the gradient color.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report


# Load dataset (replace 'data.csv' with your dataset file)
df = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')

print(df.columns.values.tolist())


# Identify target column and features
target_column = ' Label'  # Replace with your target column name
X = df.drop(target_column, axis = 1)  # Feature set
y = df[target_column]  # Target variable
df[target_column] = (df[target_column] == 'DDoS').astype(int)

# Handle categorical variables (if any)
# Convert categorical columns to numeric using LabelEncoder
for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#print(len(X_train))



# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())



# Build a Random Forest model to determine feature importance
rf = RandomForestClassifier(n_estimators=10, random_state=42)
#rf = RandomForestClassifier(n_jobs = 4)

rf.fit(X_train, y_train)

# Get feature importance
feature_importances = rf.feature_importances_
features = X.columns

# Create a DataFrame for easy interpretation
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Print the most important features
#print("Feature Importance (Random Forest):")
#print(feature_importance_df)

# Correlation Analysis (For numerical features)
correlation_matrix = df.corr()
plt.figure(figsize=(20, 40))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Features')
plt.show()

# Identify correlation with the target variable
correlation_with_target = correlation_matrix[target_column].sort_values(ascending=False)
#print("\nCorrelation with Target Variable:")
#print(correlation_with_target)

```

As Observed in the map, there are some patches off of the diagonal that appear a dark red. Close observation indicated that these feature relationships are highly correlated as they are derived from the same information source, e.g. Subflow Fwd packet and Total Fwd packets appear to be highly correlated. However, both are derived from FWD packets so the relationship does not appear statistically significant for my purposes.

## 3.2 Visualization: Feature Importance

In order to identify a subset of features to use in further analysis, I rank the importance of the features based on their respective correlation value with the target column, in this case the Label column.

I plot the ranking and print the first few values of the feature ranking below.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def feature_importance_by_correlation(csv_file, target_column):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Ensure the target column exists
    if target_column not in df.columns:
        raise ValueError(f"Target column '{target_column}' not found in CSV file.")

    df[target_column] = (df[target_column] == 'DDoS').astype(int)

    # Compute correlation matrix
    correlation_matrix = df.corr()

    # Get absolute correlation values with target variable
    feature_importance = correlation_matrix[target_column].abs().drop(target_column)

    # Sort features by importance (highest correlation first)
    feature_importance = feature_importance.sort_values(ascending=False)

    # Plot feature importance
    plt.figure(figsize=(20, 30))
    sns.barplot(x=feature_importance.values, y=feature_importance.index, palette="viridis")
    plt.xlabel("Absolute Correlation with Target")
    plt.ylabel("Features")
    plt.title("Feature Importance Based on Correlation")
    plt.show()

    return feature_importance

file_data = 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'

feature_importance_by_correlation(file_data, ' Label')


# Example usage
# file_path = "your_dataset.csv"
# target = "your_target_column"
# importance = feature_importance_by_correlation(file_path, target)
# print(importance)
```

## 4. Models

I was wary to draw any significant conclusions from the correlation matrix, so I decided to train a baseline model on the full set of features, identify which features were the most important to the model predictions, and then compare them to the ranking from the correlation matrix.

## 4.1 Models: RandomForest Classifier - full feature set

I used the randomforest classifier and trained it on the full feature set. I printed the classification report, model accuracy, and ranked the feature importance from random forest via a bar plot. I also printed the first few features in the ranking with their values for easy visualization.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}

# Evaluate the model to check its predictive power
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
print("\nClassification Report:\n", classification_rep)

print(f"\nModel Accuracy: {accuracy:.4f}")

# Optionally, visualize feature importances
plt.figure(figsize=(20, 40))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.title('Feature Importance from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Print the most important features
print("Feature Importance (Random Forest):")
print(feature_importance_df)
```

As seen above, the RandomForest achieved an accuracy rate of .999, which is startling. To fully understand Why this might be the case I decided to explore the relationships between these 'important features' in a different way. Namely, Logistic Regression.

## 4.2.1 Models: Logistic Regression - Subflow Fwd Bytes x Fwd Packet Length Max

I was curious as to whether or not any of these 'important' features had any significant relationship that would predict the classification results. I used the first and third feature to try and fit a logistic regression model.

```{python}
#| colab: {base_uri: https://localhost:8080/}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc


df2 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')



target_column = ' Label'  # Replace with your target column name
X = df2[[' Subflow Fwd Bytes', ' Fwd Packet Length Max']]
#X = df2.drop(target_column, axis = 1)  # Feature set
y = df2[target_column]  # Target variable
#print(y)
df2[target_column] = (df2[target_column] == 'BENIGN').astype(int)
#print(y)
#le = LabelEncoder()
#y = le.fit_transform(y)

for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#print(len(X_train))

# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 585}
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_test[:, 1], y=X_test[:, 0], hue=y_test, palette={
                'BENIGN': 'blue', 'DDoS': 'red'}, marker='o')
plt.xlabel("Subflow Fwd Bytes")
plt.ylabel("Fwd packet length max")
plt.title("Logistic Regression Decision Boundary\nAccuracy: {:.2f}%".format(
    accuracy * 100))
plt.legend(title="Label", loc="upper right")
plt.show()
```

The logistic regression model for these two features returned an accuracy rate of 80.67. Based on the plot and confusion matrix, however, I am inclined to believe that this fairly high accuracy result came from mainly predicting the behavior as Benign since there were no false positives. So I adjusted the logistic regression model by adding a quadratic term and making sure to balance the weights to penalize the automatic prediction of the Benign class.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc


df2 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')



target_column = ' Label'  # Replace with your target column name
X = df2[[' Subflow Fwd Bytes', ' Fwd Packet Length Max']]
#X = df2.drop(target_column, axis = 1)  # Feature set
y = df2[target_column]  # Target variable

df2[target_column] = (df2[target_column] == 'BENIGN').astype(int)

#le = LabelEncoder()
#y = le.fit_transform(y)

for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the Logistic Regression model
poly_log_model = make_pipeline(PolynomialFeatures(degree=4), StandardScaler(), LogisticRegression(class_weight='balanced'))
poly_log_model.fit(X_train, y_train)



#model = LogisticRegression()
#model.fit(X_train, y_train)

y_pred = poly_log_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```

```{python}
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_test[:, 1], y=X_test[:, 0], hue=y_test, palette={
                'BENIGN': 'blue', 'DDoS': 'red'}, marker='o')
plt.xlabel("Subflow Fwd Bytes")
plt.ylabel("Fwd packet length max")
plt.title("Logistic Regression Decision Boundary\n Balanced Weight + quadratic term \nAccuracy: {:.2f}%".format(
    accuracy * 100))
plt.legend(title="Label", loc="upper right")
plt.show()
```

While not as accurate as the 99.9 % accuracy achieved by the RandomForest Classifier, the balanced logistic regression model with a quadratic term is still able to accurately classify 85.62% of the data points using two of the top three influential features. However, the decision boundaries in the plot make me wonder if these two features actually determine benign behavior from DDoS or if its a coincidence of the dataset that most of the DDoS points have zero for these features.

## 4.2.2 Models: Logistic Regression - Subflow Fwd Bytes x Init_Win_Bytes_forward

I repeated this process using the top two features in the ranking, however the accuracy was lower and the results were not as interpretable.

```{python}

df3 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')



target_column = ' Label'  # Replace with your target column name
X = df3[[' Subflow Fwd Bytes', 'Init_Win_bytes_forward']]
#X = df2.drop(target_column, axis = 1)  # Feature set
y = df3[target_column]  # Target variable

df3[target_column] = (df2[target_column] == 'BENIGN').astype(int)

#le = LabelEncoder()
#y = le.fit_transform(y)

for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#print(len(X_train))

# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 585}
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_test[:, 1], y=X_test[:, 0], hue=y_test, palette={
                'BENIGN': 'blue', 'DDoS': 'red'}, marker='o')
plt.xlabel("Subflow Fwd Bytes")
plt.ylabel("init_win_bytes")
plt.title("Logistic Regression Decision Boundary\nAccuracy: {:.2f}%".format(
    accuracy * 100))
plt.legend(title="Label", loc="upper right")
plt.show()
```

I modified the logistic regression for these two features by balancing the weights to penalize automatic assumptions of the most frequent class and added a quadratic term.

```{python}

df3 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')



target_column = ' Label'  # Replace with your target column name
X = df3[[' Subflow Fwd Bytes', 'Init_Win_bytes_forward']]
#X = df2.drop(target_column, axis = 1)  # Feature set
y = df3[target_column]  # Target variable

df3[target_column] = (df2[target_column] == 'BENIGN').astype(int)

#le = LabelEncoder()
#y = le.fit_transform(y)

for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#print(len(X_train))

# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the Logistic Regression model
poly_log_model = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), LogisticRegression(class_weight='balanced'))
poly_log_model.fit(X_train, y_train)



#model = LogisticRegression()
#model.fit(X_train, y_train)

y_pred = poly_log_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```

```{python}
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_test[:, 1], y=X_test[:, 0], hue=y_test, palette={
                'BENIGN': 'blue', 'DDoS': 'red'}, marker='o')
plt.xlabel("Subflow Fwd Bytes")
plt.ylabel("init_win_bytes")
plt.title("Logistic Regression Decision Boundary\nBalanced Weight + quadratic term \nAccuracy: {:.2f}%".format(
    accuracy * 100))
plt.legend(title="Label", loc="upper right")
plt.show()
```

Since the plot showed no obvious change, we can look at the confusion matrix to see how this affected the classification. As we can see, compared to the confusion matrix from the previous LR with these two features where there were no false positives but thousands of false negatives, the balanced LR has a more equal distribution of both so the accuracy did not increase by much. This leads me to conclude that these two features are not enough to properly predict DDoS behavior.

It still does much worse than the balanced LR from 4.2.1 Models: Logistic Regression - Subflow Fwd Bytes x Fwd Packet Length Max (86% accuracy) and the RandomForest Classifier (99.9% accuracy)

```{python}
#| colab: {base_uri: https://localhost:8080/}
new_feats = []
for feat in range(29):
  new_feats.append(feature_importance_df.iloc[feat][0])
#print(new_feats)


#print(feature_importance_df.iloc[2][0])
```

### 4.3 Models: Feature Reduction: PCA - full feature set

After training the Random Forest classifier on the full feature set, and analyzing the relationships between the top influential features, I needed to try and narrow down the feature set in order to answer my questions. I wished to do this in a way that would allow me to visualize the clusters so I decided to perform Principle Component Analysis on the dataset as my feature reduction technique before retraining the model.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load dataset
file_path = "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"  # Change this to your CSV file path
df = pd.read_csv(file_path)

# Preprocess: Handle non-numeric columns (if any)

target_column = ' Label'  # Replace with your target column name
X = df.drop(target_column, axis = 1)  # Feature set
y = df[target_column]  # Target variable
df[target_column] = (df[target_column] == 'DDoS').astype(int)

df_numeric = df.select_dtypes(include=[np.number]).dropna()

df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)
df_numeric = df_numeric.fillna(df_numeric.mean())




# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric)

# Apply PCA
num_components = min(df_numeric.shape[0], df_numeric.shape[1])  # Ensuring valid number of components
pca = PCA(n_components=num_components)
principal_components = pca.fit_transform(scaled_data)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Plot explained variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.show()

# Create a DataFrame with principal components
columns = [f'PC{i+1}' for i in range(principal_components.shape[1])]
pca_df = pd.DataFrame(data=principal_components, columns=columns)
print(pca_df.head())
```

The analysis indicated that I could explain over 90% of the variance using 20 principle components. While this was not a small enough number I wanted to try and see how the clusters were represented.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import plotly.express as px


# Load dataset
file_path = "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"  # Change this to your CSV file path
df = pd.read_csv(file_path)

# Preprocess: Handle non-numeric columns (if any)
df_numeric = df.select_dtypes(include=[np.number])

df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)
df_numeric = df_numeric.fillna(df_numeric.mean())


target_column = " Label"

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric)

# Apply PCA
num_components = min(df_numeric.shape[0], df_numeric.shape[1])  # Ensuring valid number of components
pca = PCA(n_components=20)
principal_components = pca.fit_transform(scaled_data)

labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = px.scatter_matrix(
    principal_components,
    labels=labels,
    dimensions=range(3),
    color=df[target_column]
)
fig.update_traces(diagonal_visible=False)
fig.show()


# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Plot explained variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.show()

# Create a DataFrame with principal components
columns = [f'PC{i+1}' for i in range(principal_components.shape[1])]
pca_df = pd.DataFrame(data=principal_components, columns=columns)
print(pca_df.head())
```

### 4.4 Models: Random Forest Classifier + PCA

I trained another Random Forest classifier model on the subset of important features found by the initial model to determine how they affected its accuracy. I then used PCA on these features to try and get a better visualization of the behavior clusters. I was able to explain more of the cumulative variance with fewer principle components. I then graphed a 3d image to see a fuller view of the cluster pattern. In the 3D model you can see where the red DDoS cluster (more easily visible in the 2d scatter plots) is dilineated from the blue benign behavior.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
#| scrolled: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report


# Load dataset (replace 'data.csv' with your dataset file)
df_mod = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')

target_column = ' Label'  # Replace with target column name

total_feats = df_mod.columns.values.tolist()
ignore_cols = [x for x in total_feats if x not in new_feats]
ignore_cols1 = ignore_cols
ignore_cols.append(target_column)


# Identify target column and features
#target_column = ' Label'  # Replace with target column name
#new_feats.append(target_column)
X = df_mod.drop(ignore_cols, axis = 1)
print(X.head())  # Feature set
y = df_mod[target_column]  # Target variable
df_mod[target_column] = (df_mod[target_column] == 'DDoS').astype(int)

# Handle categorical variables (if any)
# Convert categorical columns to numeric using LabelEncoder
for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#print(len(X_train))



# *** Replace infinite values with NaN ***
X_train = X_train.replace([np.inf, -np.inf], np.nan)
# *** Impute NaN values using the mean of the column ***
X_train = X_train.fillna(X_train.mean())
X_test = X_test.replace([np.inf, -np.inf], np.nan)
X_test = X_test.fillna(X_test.mean())



# Build a Random Forest model to determine feature importance
rf_mod = RandomForestClassifier(n_estimators=10, random_state=42)
#rf = RandomForestClassifier(n_jobs = 4)

rf_mod.fit(X_train, y_train)

# Get feature importance
feature_importances = rf_mod.feature_importances_
features = X.columns

# Create a DataFrame for easy interpretation
feature_importance_df_mod = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort by importance
feature_importance_df_mod = feature_importance_df_mod.sort_values(by='Importance', ascending=False)

# Print the most important features
#print("Feature Importance (Random Forest):")
#print(feature_importance_df_mod)

"""
#Correlation Analysis (For numerical features)
correlation_matrix = df.corr()
plt.figure(figsize=(20, 40))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Features')
plt.show()

# Identify correlation with the target variable
correlation_with_target = correlation_matrix[target_column].sort_values(ascending=False)
print("\nCorrelation with Target Variable:")
print(correlation_with_target)
"""
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
# Evaluate the model to check its predictive power
y_pred = rf_mod.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
print("\nClassification Report:\n", classification_rep)

print(f"\nModel Accuracy: {accuracy:.4f}")

# Optionally, visualize feature importances
plt.figure(figsize=(20, 40))
plt.barh(feature_importance_df_mod['Feature'], feature_importance_df_mod['Importance'])
plt.title('Feature Importance from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import plotly.express as px


# Load dataset
file_path = "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"  # Change this to your CSV file path
df1 = pd.read_csv(file_path)
ignore_cols1.pop()
#print(ignore_cols1)
df = df1.drop(ignore_cols1, axis = 1)

# Preprocess: Handle non-numeric columns (if any)
df_numeric = df.select_dtypes(include=[np.number])

df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)
df_numeric = df_numeric.fillna(df_numeric.mean())


target_column = " Label"

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric)

# Apply PCA
num_components = min(df_numeric.shape[0], df_numeric.shape[1])  # Ensuring valid number of components
pca = PCA(n_components=20)
principal_components = pca.fit_transform(scaled_data)

labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = px.scatter_matrix(
    principal_components,
    labels=labels,
    dimensions=range(4),
    color=df[target_column]
)
fig.update_traces(diagonal_visible=False)
fig.show()


# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Plot explained variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.show()

# Create a DataFrame with principal components
columns = [f'PC{i+1}' for i in range(principal_components.shape[1])]
pca_df = pd.DataFrame(data=principal_components, columns=columns)
print(pca_df.head())
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 434}

from mpl_toolkits.mplot3d import Axes3D
# %matplotlib notebook

#X = data.data
#y = data.target

file_path = "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"  # Change this to your CSV file path
df1 = pd.read_csv(file_path)

df = df1.drop(ignore_cols1, axis = 1)

# Preprocess: Handle non-numeric columns (if any)
df_numeric = df.select_dtypes(include=[np.number])

df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)
df_numeric = df_numeric.fillna(df_numeric.mean())


target_column = " Label"

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_numeric)


pca = PCA(n_components=3)
pca.fit(scaled_data)
X_pca = pca.transform(scaled_data)
#y = df[target_column]  # Target variable

df[target_column] = (df[target_column] == 'DDoS').astype(int)

y = df[target_column]  # Target variable

ex_variance=np.var(X_pca,axis=0)
ex_variance_ratio = ex_variance/np.sum(ex_variance)
ex_variance_ratio


Xax = X_pca[:,0]
Yax = X_pca[:,1]
Zax = X_pca[:,2]

label_mapping = {'BENIGN': 0, 'DDoS': 1}
color_mapping = {0: 'blue',1: 'red'}

cdict = {0:'blue',1:'red'}
labl = {0:'DDos',1:'BENIGN'}
marker = {0:'*',1:'o'}
alpha = {0:.3, 1:.5}

fig = plt.figure(figsize=(7,5))
ax = fig.add_subplot(111, projection='3d')

fig.patch.set_facecolor('white')
for l in np.unique(y):
 ix=np.where(y==l)
 ax.scatter(Xax[ix], Yax[ix], Zax[ix], c=color_mapping[l], s=40,
           label=labl[l], marker=marker[l], alpha=alpha[l])
# for loop ends
ax.set_xlabel("First Principal Component", fontsize=14)
ax.set_ylabel("Second Principal Component", fontsize=14)
ax.set_zlabel("Third Principal Component", fontsize=14)

ax.legend()
plt.show()
```

## 5. Results, Analysis, Discussion

My preliminary exploration of the dataset in part 3 helped me understand how the various features may be correlated. However, due to the large number of features and the moderate correlation coefficients, no immediate relationships stuck out as important to explore. With this being the case, I decided to try and train a model using the full set of features and identify any significance that way.

Using the RandomForest Classifier I trained a classification model on the dataset using a 70/30 split. I then ranked the feature importance, as determined by the model, to identify which features were the most useful in classifying DDoS attacks.

Based on this model, I took a subset of the top ranking features to fit a logistic regression model, hoping for some significant predictive relationship. While not completely clear, the visual plots indicate that their were clear values that separated Benign from DDoS with these features, however the decison boundaries were not clear.

In my attempt to improve my logistic regression models I added a quadratic term and balanced the weights. While this helped improve the classification accuracy it did not seem to improve the decision boundary plots.

When Comparing the two approaches (RF and LR) and their respective accuracy in clasifying the data (99.9% and 86%) we can make a few assumptions about the dataset. RF is much less sensitive to noise and outliers compared to LR. During the generation of this dataset, abstract benign behavior from 25 different 'users' was added to mimic real world network traffic. This may have resulted in too much variance among the benign behavior which confused the LR model. The RF model on the otherhand is less sensitive to noise since each decision tree focuses on a subset of data.

Since the logistic regression plots were not successful in identifying the class clusters, I wished to try and visualize how I could separate the two classes more distinctly. It was to this end that I decided to perform PCA to reduce the dimensions of the data to get a clearer picture.

I ran PCA on the full set of features. Based on the explained variance plot I would need at least 20 principle components to explain more than 90% of the variance. While all the PCA scatterplots showed a distinct DDoS cluster, it was still two many components for a proper visualization.

I decided to try and run another Random Forest Classifier on the top 29 most important features derived from the initial model. This choice was made by looking at the importance values and choosing the features with a value greater than .00001. The resulting model was still able to accurately classify the data with a rate of .99. This rate is the exact same as the model based on the full feature set which indicates one of two things: either all the removed features had no bearing at all on classifying the behavior, or there is an error with my code.

Despite this I replicated my PCA on this subset of features and was able to further reduce my dimensions to get a clearer picture of the clusters. I further enhanced the 2d scatter plots by plotting a 3d visualization using the first three PC's.

My initial questions focused on finding definitive features to define DDos behavior and the associated thresholds. However, as I analyzed the data I saw more value in determining if this was possible first. To that end, I decided to try and fit various models and pick feature sets to determine whether or not we could clearly dilineate DDoS from Benign behavior. My change in direction from the discrete to the more abstract visualization came as a result of some of my research results on different network datasets.

This particular dataset is not necessarily accurate in a real world environment. Thus, using this particular dataset to identify specific features, and thereby ignoring others, will not be applicable to any other dataset or realworld IDS. Not all DDoS attacks in the realworld will be characterized by features found from this subset.

Thus, I thought a more interesting analysis would be to identify whether or not the malicious behavior followed a similar feature pattern, i.e. would it concentrate in a specific cluster?

I was able to conclude that DDoS does infact have a defined behavior pattern whereas benign is much more broad.

This more general modeling of the data allows me to determine whether or not their is a pattern to the attack behavior. The more broad examination can be applied to different CIC datasets with various other common attacks in addition to broader datasets from other network permimeter logs.

## 6. Impact

If I was able to narrow down my feature selection even further, I may be able to classify the scope that defines the classification of a DDos attack. This would allow Intrusion Detection systems to focus on specific data behavior, saving time and memory space.

If we apply this analysis to different datasets that contain various types of network attacks, We could potentially isolate the feature scopes of different malicious behavior. If my models did prove to be accurate, were applied to realistic datasets, and applied at scale, this could help network operators and IDS's reduce computation and memory costs by training their anomoly-based intrusion detection models on a smaller subset of network data.

However, ignoring data may prove dangerous. My preliminary analysis allowed me to more than halve the featue set without any significant accuracy loss. While I am also interested in trying to further narrow this subset such that I can achieve the smallest feature set possible without reducing the classification accuracy, this dataset is synthetically generated and does not represent a realworld environment despite UNB's best efforts. Thus, fine tuning my feature selection on this dataset will scale to real world implementation. Weeded out features in this environment may prove crucial on real world networks. For this reason, I did not attempt to further narrow down the feature subset.
